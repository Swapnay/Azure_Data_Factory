{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why should one use Azure Key Vault when working in the Azure environment? What are the pros and cons? What are the alternatives?\n",
    "* Azure Key Vault is a centralizing storage of application secrets in Azure Key Vault allows you to control their distribution. Key Vault reduces the chances that secrets may be accidentally leaked. When using Key Vault, application developers no longer need to store security information in their application.\n",
    "#### Services\n",
    "* Create or import a key or secret\n",
    "* Revoke or delete a key or secret\n",
    "* Authorize users or applications to access the key vault, so they can then manage or use its keys and secrets\n",
    "* Configure key usage (for example, sign or encrypt)\n",
    "* Monitor key usage\n",
    "####\n",
    "* Single point of failure.\n",
    "* there is al imit on number of get opeations before encountering a throttling HTTP status code as mentioned in https://docs.microsoft.com/en-us/azure/key-vault/general/service-limits\n",
    "* There is a limit on Private endpoints per key vault and Key vaults with private endpoints per subscription although these can be increased by contacting MS.\n",
    "#### Alternatives\n",
    "* AWS Key Management Service (KMS)\n",
    "* AWS CloudHSM.\n",
    "* Egnyte.\n",
    "* GnuPG.\n",
    "* HashiCorp Vault.\n",
    "* OpenSSH.\n",
    "* Google Cloud Key Management Service.\n",
    "* TokenEx.\n",
    "###  How do you achieve loop functionality within a Azure Data Factory pipeline? Why would you need to use this functionality in a data pipeline?\n",
    "* Using  ForEach Activity. It defines a repeating control flow in data pipeline\n",
    "* Data integration flows often involve execution of the same tasks on many similar objects. For example - copying multiple files identified using date from one folder into another or copying multiple tables from one database into another. Azure Data Factory's (ADF) ForEach and Until activities are designed to handle iterative processing logic.\n",
    "* More info https://docs.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity\n",
    "* How to https://github.com/Mmodarre/AzureDataFactoryHOL/blob/master/Lab-guide/06-Self-hosted_Integration_Runtime__decompress_files_and_Delete_activity/README.md\n",
    "### What are expressions in Azure Data Factory? How are they helpful when designing a data pipeline? Please explain with an example\n",
    "* In Data Factory,  the expression language is used for mapping data flow feature to configure data transformations. \n",
    "* The expressions are evaluated at runtime.These expressions are composed of column values, parameters, functions, operators, and literals that evaluate run time during pipeline execution.\n",
    "* For example, below expression is used to build file path,filename and filetype are dataset parametes these are passed to dataset while pipeline execution.\n",
    " @dataset().folder/@{dataset().filename}.@{dataset().filetype}\n",
    " * these can also be used in functions\n",
    " Split(Name,\",\")\n",
    " * More on expressions https://docs.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions\n",
    " ###  What are the pros and cons of parametrizing a datasetâ€™s activity in Azure DataFactory?\n",
    " * Parametrizing allows linked services and datasets to be reused.\n",
    " * Only downside i see is aded expression evaluation time.\n",
    " ### What are the different supported file formats and compression codecs in Azure Data Factory? When will you use a Parquet file over an ORC file? Why would you choose an AVRO file format over a Parquet file format?\n",
    " * Avro,Binary,DelimitedText,Excel,Json,ORC,Parquet,XML\n",
    " * Avro row oriented nature generally leads it to support OLTP wokflows.\n",
    " * choose Avro over Parquet fo OLTP workflows.\n",
    " * ORC and Parquet are natively supported in spark.ORC is better optimized for HIVE and Parquet is for Spark.\n",
    " * Parquet supports storing querying  deeply nested data.And is better suites for analytical querying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
